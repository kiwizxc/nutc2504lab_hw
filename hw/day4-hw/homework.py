import os
import json
from urllib.parse import unquote  # å¼•å…¥è§£ç¢¼å·¥å…·
from typing import TypedDict, Annotated, Literal, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, END, add_messages

# åŒ¯å…¥å·¥å…·æ¨¡çµ„ (è«‹ç¢ºä¿ search_searxng.py å’Œ vlm_read_website.py åœ¨åŒç›®éŒ„ä¸‹)
from search_searxng import search_searxng
from vlm_read_website import vlm_read_website

# ============ é…ç½®å€ ============
# ä½¿ç”¨ ws-03 ä¼ºæœå™¨
llm = ChatOpenAI(
    base_url="https://ws-03.wade0426.me/v1",
    api_key="EMPTY",
    model="/models/gpt-oss-120b",
    temperature=0
)

CACHE_FILE = "verification_cache.json"

# ============ 1. å®šç¾©ç‹€æ…‹ (State) ============
class GraphState(TypedDict):
    question: str
    knowledge_base: str
    messages: Annotated[list[BaseMessage], add_messages]
    loop_count: int
    is_cache_hit: bool
    final_answer: str
    visited_urls: List[str] # è¨˜éŒ„å·²è®€éçš„ç¶²å€

# ============ 2. å¿«å–å·¥å…· ============
def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return {}
    return {}

def save_cache(question, answer):
    data = load_cache()
    data[question] = answer
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# ============ 3. å®šç¾©ç¯€é» (Nodes) ============

def check_cache_node(state: GraphState):
    print("\n--- 1. æª¢æŸ¥å¿«å– (Cache Check) ---")
    data = load_cache()
    
    if state["question"] in data:
        print("âœ… å‘½ä¸­å¿«å–ï¼æº–å‚™ç›´æ¥è¼¸å‡ºã€‚")
        return {
            "is_cache_hit": True, 
            "final_answer": data[state["question"]],
            "knowledge_base": "",
            "loop_count": 0,
            "visited_urls": []
        }
    
    print("âŒ æœªå‘½ä¸­ï¼Œé€²å…¥æŸ¥è­‰æµç¨‹ã€‚")
    return {
        "is_cache_hit": False, 
        "knowledge_base": "", 
        "loop_count": 0,
        "visited_urls": []
    }

# ã€ä¸»è¦ä¿®æ”¹è™•ã€‘Planner ç¾åœ¨æœƒå°å‡ºç†ç”±
def planner_node(state: GraphState):
    print(f"\n--- 2. æ±ºç­–ä¸­ (Planner) [Loop: {state['loop_count']}] ---")
    
    if state["loop_count"] >= 4:
        print("âš ï¸ å·²é”æœ€å¤§æœå°‹æ¬¡æ•¸ï¼Œå¼·åˆ¶é€²è¡Œå›ç­”ã€‚")
        return {"messages": [AIMessage(content="Planneræ±ºç­–: ENOUGH")]}

    # ä¿®æ”¹ Promptï¼šè¦æ±‚ AI å…ˆçµ¦ç†ç”±ï¼Œå†çµ¦æ±ºç­–
    prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['question']}
    
    ç›®å‰å·²æ”¶é›†çš„å¤–éƒ¨è³‡è¨Š(Knowledge Base):
    {state.get('knowledge_base', 'å°šç„¡è³‡è¨Š')}
    
    è«‹åˆ¤æ–·ï¼šç›®å‰çš„è³‡è¨Šæ˜¯å¦å·²ç¶“ã€Œè¶³å¤ ã€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Ÿ
    
    è«‹ä¾ç…§ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
    ç†ç”±ï¼š(è«‹ç°¡çŸ­èªªæ˜é‚„ç¼ºå°‘ä»€éº¼é—œéµæ•¸æ“šã€å¹´ä»½æˆ–æ˜¯ç´°ç¯€ï¼Œæˆ–è€…ç‚ºä»€éº¼è³‡è¨Šå·²è¶³å¤ )
    æ±ºç­–ï¼š(æœ€å¾Œä¸€è¡Œè«‹å‹™å¿…åªè¼¸å‡º "SEARCH" æˆ– "ENOUGH")
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    content = response.content.strip()
    
    # å°å‡º AI çš„æ€è€ƒéç¨‹
    print(f"ğŸ¤” åˆ¤æ–·ç†ç”±:\n{content}")

    # è§£æé‚è¼¯ (æŠ“å–æœ€å¾Œä¸€è¡Œçš„æ±ºç­–)
    lines = content.split('\n')
    last_line = lines[-1].upper()
    
    # ç°¡å–®çš„é˜²å‘†åˆ¤æ–·ï¼Œå¦‚æœæœ€å¾Œä¸€è¡ŒåŒ…å« SEARCH æˆ– ENOUGH å°±æŠ“å–
    if "SEARCH" in last_line or "SEARCH" in content.split("æ±ºç­–ï¼š")[-1]:
        decision = "SEARCH"
    else:
        decision = "ENOUGH"
    
    print(f"ğŸ¤– Planner æœ€çµ‚æ±ºå®š: {decision}")
    return {"messages": [AIMessage(content=f"Planneræ±ºç­–: {decision}")]}

def query_gen_node(state: GraphState):
    print("\n--- 3. ç”Ÿæˆæœå°‹é—œéµå­— (Query Gen) ---")
    prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['question']}
    ç›®å‰å·²çŸ¥è³‡è¨Š: {state.get('knowledge_base', '')}
    
    è«‹ç”Ÿæˆä¸€å€‹ã€Œæœ€é©åˆæœå°‹å¼•æ“ã€çš„é—œéµå­—ï¼Œç”¨ä¾†æŸ¥æ‰¾ç¼ºå°‘çš„è³‡è¨Šã€‚
    åªå›è¦†é—œéµå­—æœ¬èº«ï¼Œä¸è¦åŠ ä»»ä½•æ¨™é»ç¬¦è™Ÿã€‚
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    query = response.content.strip()
    print(f"ğŸ”‘ ç”Ÿæˆé—œéµå­—: {query}")
    return {"messages": [AIMessage(content=query)]}

def search_and_read_node(state: GraphState):
    query = state["messages"][-1].content
    print(f"\n--- 4. åŸ·è¡Œæœå°‹èˆ‡é–±è®€ (Search & VLM) ---")
    
    results = search_searxng(query, limit=5)
    
    target_result = None
    # å°‡å·²è¨ªå•éçš„ç¶²å€è§£ç¢¼
    visited_normalized = [unquote(u) for u in state.get("visited_urls", [])]
    
    if results:
        for res in results:
            res_url_norm = unquote(res['url'])
            
            if res_url_norm not in visited_normalized:
                target_result = res
                break
            else:
                print(f"ğŸ™ˆ è·³éå·²è®€éçš„ç¶²å€: {res['title']}")
    
    new_info = ""
    new_visited_url = []
    
    if target_result:
        title = target_result['title']
        url = target_result['url']
        print(f"ğŸŒ é–å®šç¶²é : {title}")
        print(f"ğŸ”— URL: {url}")
        
        print("ğŸ“¸ VLM æ­£åœ¨é–±è®€ç¶²é  (è«‹ç¨å€™)...")
        # æ³¨æ„: é€™è£¡æœƒå‘¼å«å¤–éƒ¨æª”æ¡ˆ
        content = vlm_read_website(url, title)
        
        new_info = f"\n=== æ–°å¢è³‡æ–™ä¾†æº: {title} ===\n{content}\n"
        new_visited_url = [unquote(url)]
    else:
        print("âš ï¸ æœå°‹çµæœçš†å·²è®€éï¼Œæˆ–ç„¡ç›¸é—œçµæœã€‚")
        new_info = "\n[ç³»çµ±] æ­¤é—œéµå­—æŸ¥ç„¡æ–°è³‡æ–™ï¼Œè«‹å˜—è©¦å…¶ä»–æ–¹å‘ã€‚\n"

    return {
        "knowledge_base": state.get("knowledge_base", "") + new_info,
        "loop_count": state["loop_count"] + 1,
        "visited_urls": state.get("visited_urls", []) + new_visited_url
    }

def answer_node(state: GraphState):
    if state.get("is_cache_hit"):
        return {}

    print("\n--- 5. ç”Ÿæˆæœ€çµ‚å›ç­” (Final Answer) ---")
    prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['question']}
    
    é€™æ˜¯ä½ è¾›è‹¦æŸ¥è­‰å¾Œæ”¶é›†åˆ°çš„è³‡è¨Š:
    {state['knowledge_base']}
    
    è«‹æ ¹æ“šä¸Šè¿°è³‡è¨Šï¼Œå®Œæ•´ä¸”å°ˆæ¥­åœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    ä¸¦åœ¨æœ€å¾Œé™„ä¸Šåƒè€ƒä¾†æºã€‚
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    
    save_cache(state["question"], response.content)
    print("ğŸ’¾ å·²å°‡çµæœå¯«å…¥å¿«å–ã€‚")
    
    return {"final_answer": response.content}

# ============ 4. å®šç¾©è·¯ç”± (Router) ============

def cache_router(state: GraphState) -> Literal["hit", "miss"]:
    if state.get("is_cache_hit"): return "hit"
    return "miss"

def planner_router(state: GraphState) -> Literal["answer", "query"]:
    last_msg = state["messages"][-1].content
    if "ENOUGH" in last_msg: return "answer"
    return "query"

# ============ 5. çµ„è£ Graph ============

workflow = StateGraph(GraphState)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_and_read_node)
workflow.add_node("final_answer", answer_node)

workflow.set_entry_point("check_cache")

workflow.add_conditional_edges(
    "check_cache",
    cache_router,
    {
        "miss": "planner",      
        "hit": "final_answer"   
    }
)

workflow.add_conditional_edges(
    "planner",
    planner_router,
    {
        "query": "query_gen",    
        "answer": "final_answer" 
    }
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")
workflow.add_edge("final_answer", END)

app = workflow.compile()

# ============ 6. åŸ·è¡Œå€ ============
if __name__ == "__main__":
    
    # å˜—è©¦ç¹ªè£½ ASCII æµç¨‹åœ–
    try:
        print(app.get_graph().draw_ascii())
    except Exception:
        pass

    print(f"ğŸš€ è‡ªå‹•æŸ¥è­‰ AI å·²å•Ÿå‹•ï¼(Model: /models/gpt-oss-120b)")
    
    while True:
        try:
            q = input("\nè«‹è¼¸å…¥æƒ³æŸ¥è­‰çš„å•é¡Œ (q é›¢é–‹): ").strip()
            
            if q.lower() == 'q': break
            
            if not q:
                print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œï¼")
                continue
            
            inputs = {"question": q, "messages": []}
            result = app.invoke(inputs)
            
            print("\n" + "="*30)
            print("ğŸ’¡ æœ€çµ‚çµæœ:")
            print(result["final_answer"])
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")