import os
import csv
import requests
import re
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct

# --- 1. ç’°å¢ƒé…ç½® ---
client = QdrantClient(host="localhost", port=6333)
collection_name = "cw02_final_collection"
API_URL = "https://ws-04.wade0426.me/embed"

# å»ºç«‹å‡è³‡æ–™ï¼šå¦‚æœæ²’æœ‰ text.txtï¼Œå»ºç«‹ä¸€å€‹
if not os.path.exists("text.txt"):
    with open("text.txt", "w", encoding="utf-8") as f:
        f.write("é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ã€‚å®ƒç”¨ä¾†æ¸¬è©¦åˆ‡å¡ŠåŠŸèƒ½ã€‚\n\né€™æ˜¯ç¬¬äºŒæ®µè½ï¼Œç”¨ä¾†æ¸¬è©¦èªæ„åˆ†å‰²çš„æ•ˆæœã€‚")

# å»ºç«‹å‡è³‡æ–™ï¼šå¦‚æœæ²’æœ‰ table è³‡æ–™å¤¾æˆ–è£¡é¢æ²’æª”æ¡ˆï¼Œå»ºç«‹ä¸€å€‹ csv æ¸¬è©¦
if not os.path.exists("table"):
    os.makedirs("table")
    
if not os.listdir("table"):
    csv_content = [
        ["ç”¢å“", "åƒ¹æ ¼", "åº«å­˜", "å‚™è¨»"],
        ["è˜‹æœ", "30", "100", "æ–°é®®åˆ°è²¨"],
        ["é¦™è•‰", "15", "50", "ä¾†è‡ªæ——å±±"],
        ["æ©˜å­", "25", "80", "å­£ç¯€é™å®š"]
    ]
    with open("table/sample_table.csv", "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerows(csv_content)
    print("âš ï¸ åµæ¸¬åˆ° table è³‡æ–™å¤¾ç‚ºç©ºï¼Œå·²è‡ªå‹•å»ºç«‹ sample_table.csv ä¾›æ¸¬è©¦ç”¨ã€‚")

# è®€å–ä¸»è¦æ–‡æœ¬
try:
    with open("text.txt", "r", encoding="utf-8") as f:
        source_text = f.read()
except FileNotFoundError:
    source_text = ""

# --- 2. å¯¦ä½œï¼šå›ºå®šåˆ‡å¡Š (Fixed-size Chunking) ---
def fixed_size_chunking(text, chunk_size=200):
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks

# --- 3. å¯¦ä½œï¼šèªæ„æ»‘å‹•è¦–çª— (ç¬¦åˆåœ–ç‰‡ä¸‹åŠéƒ¨è¦æ±‚) ---
def semantic_sliding_window(text, chunk_size=250, overlap=50):
    chunks = []
    sentences = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n+)', text)
    
    current_chunk = ""
    combined_sentences = []
    temp_sent = ""
    for s in sentences:
        temp_sent += s
        if re.search(r'(ã€‚|ï¼|ï¼Ÿ|\n+)', s) or len(s.strip()) == 0:
            if temp_sent.strip():
                combined_sentences.append(temp_sent)
            temp_sent = ""
    if temp_sent: combined_sentences.append(temp_sent)

    for sentence in combined_sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence
        else:
            chunks.append(current_chunk)
            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
            current_chunk = overlap_text + sentence
            
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

# --- 4. [å¼·åŒ–ç‰ˆ] å¯¦ä½œï¼šè™•ç† table è³‡æ–™å¤¾ ---
def csv_to_markdown_table(file_path):
    """å°‡ CSV è½‰æ›ç‚º Markdown è¡¨æ ¼å­—ä¸²ï¼Œè®“ LLM æ›´å®¹æ˜“ç†è§£"""
    try:
        with open(file_path, "r", encoding="utf-8", newline="") as f:
            reader = csv.reader(f)
            rows = list(reader)
            if not rows: return ""
            
            # è£½ä½œ Markdown Header
            header = "| " + " | ".join(rows[0]) + " |"
            separator = "| " + " | ".join(["---"] * len(rows[0])) + " |"
            
            # è£½ä½œå…§å®¹
            body = []
            for row in rows[1:]:
                body.append("| " + " | ".join(row) + " |")
                
            return f"{header}\n{separator}\n" + "\n".join(body)
    except Exception as e:
        print(f"CSV è§£æå¤±æ•—: {e}")
        return ""

def process_table_folder(folder_path="table"):
    table_chunks = []
    if not os.path.exists(folder_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ° {folder_path} è³‡æ–™å¤¾ï¼Œè·³éè¡¨æ ¼è™•ç†")
        return []
    
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {folder_path} è³‡æ–™å¤¾...")
    
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        content = ""
        
        # å¿½ç•¥ç³»çµ±æª”æ¡ˆ
        if filename.startswith("."): continue
        
        try:
            # é‡å°ä¸åŒå‰¯æª”ååšè™•ç†
            if filename.lower().endswith(".csv"):
                # CSV è½‰ Markdown
                raw_csv = csv_to_markdown_table(file_path)
                if raw_csv:
                    content = raw_csv
                    print(f"  - å·²è½‰æ› CSV: {filename}")
            
            elif filename.lower().endswith((".html", ".md", ".txt")):
                # ç´”æ–‡å­—é¡ç›´æ¥è®€å–
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    print(f"  - å·²è®€å–æ–‡å­—æª”: {filename}")
            
            else:
                print(f"  - è·³éä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {filename}")
                continue

            if content:
                # åŠ ä¸Šä¾†æºæ¨™ç¤ºï¼Œé€™å° RAG å¾ˆé‡è¦
                formatted_content = f"ã€è¡¨æ ¼ä¾†æº: {filename}ã€‘\n{content}"
                table_chunks.append(formatted_content)
                
        except Exception as e:
            print(f"è®€å– {filename} å‡ºéŒ¯: {e}")
            
    return table_chunks

# --- 5. å‘é‡åŒ–å‡½å¼ ---
def get_embeddings(texts):
    if not texts: return []
    batch_size = 5
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        payload = {
            "texts": batch,
            "task_description": "æª”æ¡ˆåˆ†å¡Šèˆ‡è¡¨æ ¼æª¢ç´¢å¯¦ä½œ",
            "normalize": True
        }
        try:
            response = requests.post(API_URL, json=payload)
            if response.status_code == 200:
                all_embeddings.extend(response.json()["embeddings"])
            else:
                print(f"Embedding API Error: {response.text}")
                # å¤±æ•—æ™‚å›å‚³ç©ºå‘é‡ (é™¤éŒ¯ç”¨)
                all_embeddings.extend([[0.0]*4096] * len(batch))
        except Exception as e:
            print(f"API Connection Error: {e}")
            all_embeddings.extend([[0.0]*4096] * len(batch))
            
    return all_embeddings

# --- 6. è¼”åŠ©å‡½å¼ï¼šåŸ·è¡Œè©•ä¼° ---
def evaluate_method(method_name, chunks, query_text):
    if not chunks:
        print(f"[{method_name}] æ²’æœ‰å€å¡Šå¯è™•ç†ï¼Œè·³éã€‚")
        return

    print(f"\nğŸ§ª æ­£åœ¨è©•ä¼°æ–¹æ³•: {method_name} (å…± {len(chunks)} å€‹å€å¡Š)")
    
    vectors = get_embeddings(chunks)
    
    temp_col = f"temp_{method_name}"
    if client.collection_exists(temp_col):
        client.delete_collection(temp_col)
    
    client.create_collection(
        collection_name=temp_col,
        vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
    )
    
    points = [
        PointStruct(id=i, vector=vectors[i], payload={"text": chunks[i]})
        for i in range(len(chunks))
    ]
    client.upsert(collection_name=temp_col, points=points)
    
    query_vec = get_embeddings([query_text])[0]
    results = client.query_points(collection_name=temp_col, query=query_vec, limit=1).points
    
    if results:
        preview_text = results[0].payload['text'][:50].replace('\n', ' ')
        print(f"   ğŸ‘‰ æª¢ç´¢çµæœ: {preview_text}...")
        print(f"   ğŸ‘‰ åˆ†æ•¸ (Score): {results[0].score:.4f}")
    else:
        print("   ğŸ‘‰ ç„¡æœå°‹çµæœ")

# ================= åŸ·è¡Œæµç¨‹ =================

if __name__ == "__main__":
    # 1. æº–å‚™åˆ‡å¡Šè³‡æ–™
    print("--- 1. åŸ·è¡Œåˆ‡å¡Š ---")
    fixed_chunks = fixed_size_chunking(source_text)
    semantic_chunks = semantic_sliding_window(source_text)
    
    # é€™è£¡æœƒä½¿ç”¨å¼·åŒ–ç‰ˆçš„è¡¨æ ¼è™•ç†å‡½å¼
    table_chunks = process_table_folder("table")

    # 2. æ¯”è¼ƒå…©ç¨®åˆ‡å¡Šæ–¹æ³•
    print("\n--- 2. æ¯”è¼ƒåˆ‡å¡Šæ–¹æ³• ---")
    test_query = "è«‹èªªæ˜æœ¬æ–‡çš„æ ¸å¿ƒé‡é»æ˜¯ä»€éº¼ï¼Ÿ"
    
    evaluate_method("å›ºå®šåˆ‡å¡Š(Fixed)", fixed_chunks, test_query)
    evaluate_method("æ»‘å‹•è¦–çª—åˆ‡å¡Š(sliding+Semantic)", semantic_chunks, test_query)

    # 3. å»ºç«‹æœ€çµ‚ä½œæ¥­è³‡æ–™åº«
    print("\n--- 3. å»ºç«‹æœ€çµ‚è³‡æ–™åº« (CW/02) ---")
    final_chunks = semantic_chunks + table_chunks

    if final_chunks:
        print(f"ğŸš€ æ­£åœ¨å¯«å…¥ {len(final_chunks)} ç­†è³‡æ–™åˆ° {collection_name}...")
        final_vectors = get_embeddings(final_chunks)

        if client.collection_exists(collection_name):
            client.delete_collection(collection_name)

        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
        )

        final_points = [
            PointStruct(id=i, vector=final_vectors[i], payload={"text": final_chunks[i]})
            for i in range(len(final_chunks))
        ]

        client.upsert(collection_name=collection_name, points=final_points)
        print(f"âœ… è³‡æ–™å·²å­˜å…¥ {collection_name}")

        # 4. [æ–°å¢] é‡å°è¡¨æ ¼çš„å°ˆå±¬æ¸¬è©¦
        print("\n--- 4. è¡¨æ ¼æª¢ç´¢æ¸¬è©¦ ---")
        # å¦‚æœä½ å‰›å‰›è‡ªå‹•ç”Ÿæˆäº†é¦™è•‰çš„è³‡æ–™ï¼Œé€™è£¡æ‡‰è©²è¦èƒ½æœåˆ°
        table_query = "é¦™è•‰çš„åƒ¹æ ¼èˆ‡åº«å­˜æ˜¯å¤šå°‘ï¼Ÿ" 
        print(f"æ¸¬è©¦å•é¡Œ: {table_query}")
        
        t_vec = get_embeddings([table_query])[0]
        res = client.query_points(collection_name=collection_name, query=t_vec, limit=1).points
        if res:
            table_preview = res[0].payload['text'][:100].replace('\n', ' ')
            print(f"ğŸ‘‰ æœå°‹çµæœ: {table_preview}...")
            print(f"ğŸ‘‰ åˆ†æ•¸: {res[0].score:.4f}")
        else:
            print("âŒ æ‰¾ä¸åˆ°è¡¨æ ¼ç›¸é—œè³‡æ–™")
    else:
        print("âš ï¸ æ²’æœ‰ä»»ä½•è³‡æ–™å¡Šè¢«ç”¢ç”Ÿï¼Œè«‹æª¢æŸ¥ source_text æˆ– table è³‡æ–™å¤¾")